import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from transformers import AutoModelForCausalLM

# =========================
# è®¾å¤‡é€‰æ‹©
# =========================
# è‡ªåŠ¨æ£€æµ‹æ˜¯å¦æ”¯æŒ Apple Silicon GPU (MPS)
if torch.backends.mps.is_available():
    device = torch.device("mps")
    device_id = 0  # pipeline é‡Œç”¨ 0 è¡¨ç¤ºå¯ç”¨ GPU
    print("Using Apple Silicon GPU (MPS)")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    device_id = 0
    print("Using NVIDIA CUDA GPU")
else:
    device = torch.device("cpu")
    device_id = -1
    print("Using CPU")

# -----------------------
# Step 1. åˆ¤åˆ«å¼æ¨¡å‹ (Erlangshen)
# -----------------------
sentiment_model_name = "IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment"  # è½»é‡ç‰ˆ
tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)
model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)
model.to(device)
sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=device_id)

# -----------------------
# Step 2. ç”Ÿæˆå¼å¤§æ¨¡å‹ (Qwen)
# -----------------------
gen_model_name = "IDEA-CCNL/Erlangshen-Roberta-330M-NLI"
# gen_model_name = "Qwen/Qwen1.5-1.8B-Chat"  # å»ºè®®å°æ¨¡å‹è·‘ Demoï¼Œ7B ä¼šçˆ†å†…å­˜
gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)
gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name)
gen_model.to(device)

import json
def explain_trigger(sentence, candidate_labels):
    """ç”¨ç”Ÿæˆæ¨¡å‹è§£é‡Šè§¦å‘åŸå› ï¼Œè¿”å› JSON æ ¼å¼"""
    prompt = f"""
è¯·åˆ†æä¸‹é¢å¥å­å¯èƒ½å¼•å‘å¯¹è¯å†²çªæˆ–æƒ…ç»ªå‡çº§çš„åŸå› ã€‚å€™é€‰æ ‡ç­¾å¦‚ä¸‹ï¼š
{candidate_labels}

å¥å­ï¼š{sentence}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{
  "triggers": [
    {{
      "label": "æ ‡ç­¾å",
      "reason": "è§¦å‘åŸå› çš„è§£é‡Š",
      "confidence": "é«˜/ä¸­/ä½ æˆ– æ•°å€¼"
    }}
  ]
}}
"""

    # æ„å»ºè¾“å…¥
    inputs = gen_tokenizer(prompt, return_tensors="pt").to(gen_model.device)

    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆ
    outputs = gen_model.generate(**inputs, max_new_tokens=256)

    # è§£ç æ–‡æœ¬
    raw_output = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)

    # å°è¯•è§£ææˆ JSON
    try:
        parsed_output = json.loads(raw_output)
        return parsed_output
    except json.JSONDecodeError:
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°±è¿”å›åŸå§‹æ–‡æœ¬ï¼Œé¿å…æŠ¥é”™
        return {"raw_output": raw_output}

# def explain_trigger(sentence, candidate_labels):
#     """ç”¨ç”Ÿæˆæ¨¡å‹è§£é‡Šè§¦å‘åŸå› """
#     prompt = f"""
# è¯·åˆ†æä¸‹é¢å¥å­å¯èƒ½å¼•å‘å¯¹è¯å†²çªæˆ–æƒ…ç»ªå‡çº§çš„åŸå› ã€‚å€™é€‰æ ‡ç­¾å¦‚ä¸‹ï¼š
# {candidate_labels}
#
# å¥å­ï¼š{sentence}
#
# è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
# å¯èƒ½çš„è§¦å‘åŸå› ï¼š
# - æ ‡ç­¾ (ç½®ä¿¡åº¦æˆ–åˆç†æ€§è¯´æ˜)
# """
#     inputs = gen_tokenizer(prompt, return_tensors="pt").to(gen_model.device)
#     outputs = gen_model.generate(**inputs, max_new_tokens=256)
#     return gen_tokenizer.decode(outputs[0], skip_special_tokens=True)

# -----------------------
# Step 3. Demo æµç¨‹
# -----------------------
# conversation = [
#     "æˆ‘è§‰å¾—è‡ªå·±å¥½å¯æ€œï¼Œå¤©å¤©ä¸€ä¸ªäººåŠ ç­ã€‚",
#     "ä½ ä»–å¦ˆçš„é—­å˜´ï¼"
# ]
conversation = [
    "Cathyï¼šå…¶å®å¥½å¤šéƒ½è¯´ä¸é€šçš„ï¼Œè¿™é‡Œä¹Ÿæœ‰äººsuvå·¥ç­¾å»¶æœŸè¢«æ‹’çš„ï¼Œä¸çŸ¥é“è¿™ä¸ªæ€ä¹ˆå¼„ã€‚ä½ è§‰å¾—å‘¢ï¼Ÿ",
    "æˆ‘ï¼šæœ‰ä¸åˆç†ï¼Œå¯ä»¥ç”³è¯‰æˆ–é‡æ–°æäº¤ç”³è¯·ï¼Œä½†è¿˜æ˜¯è¦çœ‹å®¡æ ¸å®˜å’Œè¿æ°”ã€‚æ²¡æœ‰ä¸€æ¡100%çš„è·¯å¾„ã€‚",
    "Cathyï¼šæˆ‘ä¹Ÿå¯ä»¥ä¸ç”³è¯·å·¥ç­¾å¯¹æˆ‘è€Œè¨€æ²¡æœ‰æ„ä¹‰ã€‚",
    'æˆ‘ï¼šè¿™ä¸ªä½ å¯ä»¥å†è€ƒè™‘ä¸€ä¸‹ï¼Œæœ‰å†³å®šäº†æˆ‘ä»¬å¼€ä¸ªè‚¡ä¸œä¼šè®®ã€‚',
    "Cathyï¼šæˆ‘å·¥ç­¾è¢«æ‹’å’Œè‚¡ä¸œå¤§ä¼šæœ‰å•¥å…³è”ï¼Œè¦ä¸æ‰¾äººæŠŠæˆ‘æ›¿äº†ï¼Œçœå¾—é‚£ä¹ˆå¤šéº»çƒ¦ï¼Œæ—…æ¸¸ç­¾ç®€ç®€å•å•ï¼Œå°å­©è¯»å¤§å­¦æˆ‘ä¹Ÿå¯ä»¥å›å»äº†ã€‚",
    "Cathyï¼šéƒ½æ¥çœ‹æˆ‘ç¬‘è¯ï¼Œæˆ‘è¿æ°”å·®ã€‚"
]

# 1) åˆ¤åˆ«å¼æ¨¡å‹é€å¥æƒ…ç»ªåˆ†ç±»
results = sentiment_analyzer(conversation)

print("ğŸŸ¡ æƒ…ç»ªæ£€æµ‹ç»“æœï¼š")
for i, (sent, res) in enumerate(zip(conversation, results)):
    print(f"- ç¬¬{i+1}å¥: {sent} â†’ {res['label']} ({res['score']:.2f})")

# 2) è§¦å‘ç‚¹æ£€æµ‹ï¼ˆç®€å•ç‰ˆï¼šçœ‹æœ€åä¸€å¥ vs å‰é¢å¥å­çš„å·®å¼‚ï¼‰
last_res = results[-1]
trigger_idx, max_shift = -1, 0
for i in range(len(results)-1):
    shift = last_res['score'] - results[i]['score'] if results[i]['label'] != "NEGATIVE" else 0
    if shift > max_shift:
        max_shift, trigger_idx = shift, i

# 3) å¦‚æœæ‰¾åˆ°è§¦å‘ç‚¹ â†’ è°ƒç”¨ç”Ÿæˆå¼å¤§æ¨¡å‹è§£é‡Š
if trigger_idx >= 0:
    trigger_sentence = conversation[trigger_idx]
    print(f"\nâš ï¸ è§¦å‘ç‚¹å¯èƒ½æ˜¯ç¬¬ {trigger_idx+1} å¥: \"{trigger_sentence}\"")

    candidate_labels = [
    "è´£å¤‡æˆ–æŒ‡è´£",
    "è¯­æ°”ä¸è€çƒ¦",
    "ç¼ºä¹å…³å¿ƒæˆ–æ”¯æŒ",
    "è¡¨è¾¾æ— å¥ˆå’ŒæŠ±æ€¨",
    "è¡¨è¾¾æ¨¡ç³Šï¼Œå®¹æ˜“è¢«è¯¯è§£",
    "å¸¦æœ‰æ‰¹è¯„æ„å‘³"
  ]
    explanation = explain_trigger(trigger_sentence, candidate_labels)
    # print(json.dumps(explanation, indent=2, ensure_ascii=False))
    # å¦‚æœæ¨¡å‹è¿”å›äº†ç»“æ„åŒ– triggers
    if "triggers" in explanation:
        print("å¯èƒ½çš„è§¦å‘åŸå› ï¼š")
        for trig in explanation["triggers"]:
            label = trig.get("label", "æœªçŸ¥æ ‡ç­¾")
            reason = trig.get("reason", "")
            confidence = trig.get("confidence", "")
            print(f"- {label} ({confidence})ï¼š{reason}")
    else:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹è¾“å‡º
        print("æ¨¡å‹åŸå§‹è¾“å‡ºï¼š")
        print(explanation.get("raw_output", explanation))
else:
    print("\nâš ï¸ æœªèƒ½æ‰¾åˆ°æ˜æ˜¾è§¦å‘ç‚¹ã€‚")